{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"from copy import deepcopy\nimport json\nimport random\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torch.nn import functional as fnn\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nnp.random.seed(2205)\ntorch.manual_seed(2205)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Dataset, который будет использован для обучения сети поиска bbox'ов номеров - это первый этап\n## 1. Dataset for bbox detection training - 1st step****"},{"metadata":{"trusted":false},"cell_type":"code","source":"class CarPlatesDatasetWithRectangularBoxes(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9):\n        super(CarPlatesDatasetWithRectangularBoxes, self).__init__()\n        self.root = Path(root)\n        self.train_size = train_size\n        \n        self.image_names = []\n        self.image_ids = []\n        self.image_boxes = []\n        self.image_texts = []\n        self.box_areas = []\n        \n        self.transforms = transforms\n        \n        if split in ['train', 'val']:\n            plates_filename = self.root / 'train.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            train_valid_border = int(len(json_data) * train_size) + 1 # граница между train и valid\n            data_range = (0, train_valid_border) if split == 'train' \\\n                else (train_valid_border, len(json_data))\n            self.load_data(json_data[data_range[0]:data_range[1]]) # загружаем названия файлов и разметку\n            return\n\n        if split == 'test':\n            plates_filename = self.root / 'submission.csv'\n            self.load_test_data(plates_filename, split, train_size)\n            return\n\n        raise NotImplemented(f'Unknown split: {split}')\n        \n    def load_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            if sample['file'] == 'train/25632.bmp':\n                continue\n            self.image_names.append(self.root / sample['file'])\n            self.image_ids.append(torch.Tensor([i]))\n            boxes = []\n            texts = []\n            areas = []\n            for box in sample['nums']:\n                points = np.array(box['box'])\n                x_0 = np.min([points[0][0], points[3][0]])\n                y_0 = np.min([points[0][1], points[1][1]])\n                x_1 = np.max([points[1][0], points[2][0]])\n                y_1 = np.max([points[2][1], points[3][1]])\n                boxes.append([x_0, y_0, x_1, y_1])\n                texts.append(box['text'])\n                areas.append(np.abs(x_0 - x_1) * np.abs(y_0 - y_1))\n            boxes = torch.FloatTensor(boxes)\n            areas = torch.FloatTensor(areas)\n            self.image_boxes.append(boxes)\n            self.image_texts.append(texts)\n            self.box_areas.append(areas)\n        \n    \n    def load_test_data(self, plates_filename, split, train_size):\n        df = pd.read_csv(plates_filename, usecols=['file_name'])\n        for row in df.iterrows():\n            self.image_names.append(self.root / row[1][0])\n        self.image_boxes = None\n        self.image_texts = None\n        self.box_areas = None\n         \n    \n    def __getitem__(self, idx):\n        target = {}\n        if self.image_boxes is not None:\n            boxes = self.image_boxes[idx].clone()\n            areas = self.box_areas[idx].clone()\n            num_boxes = boxes.shape[0]\n            target['boxes'] = boxes\n            target['area'] = areas\n            target['labels'] = torch.LongTensor([1] * num_boxes)\n            target['image_id'] = self.image_ids[idx].clone()\n            target['iscrowd'] = torch.Tensor([False] * num_boxes)\n#             target['texts'] = self.image_texts[idx]\n\n        image = cv2.imread(str(self.image_names[idx]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transforms is not None:\n            image = self.transforms(image)\n        return image, target\n\n    def __len__(self):\n        return len(self.image_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef create_model(device):\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # replace the classifier with a new one, that has\n    # num_classes which is user-defined\n    num_classes = 2  # 1 class (person) + background\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model.to(device)\n\n# Вспомогательная функция для создания dataloader'а\ndef collate_fn(batch):\n    return tuple(zip(*batch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"transformations= transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n                    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = create_model(device)\n\n# use our dataset and defined transformations\ntrain_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'train')\nval_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'val')\ntest_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'test')\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=2, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Обучаем модель 1 эпоху\n## 2. Train the model - 1 epoch"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Часть кода взята из  pytorch utils\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    model.train()\n\n    for images, targets in tqdm.tqdm(train_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    \n    batch_losses = []\n    for images, targets in tqdm.tqdm(val_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        batch_losses.append(losses.item())\n        optimizer.zero_grad()\n    \n    batch_losses = np.array(batch_losses)\n    batch_losses = batch_losses[np.isfinite(batch_losses)]\n    print(f'Valid_loss: {np.mean(batch_losses)}')\n    lr_scheduler.step()\n\nprint(\"That's it!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"print(f'Validation average loss: {np.mean(batch_losses)}')","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'np' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-aa4c104f5712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Validation average loss: {np.mean(batch_losses)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Save\n# with open('fasterrcnn_resnet50_fpn_1_epoch', 'wb') as fp:\n#     torch.save(model.state_dict(), fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load\nwith open('fasterrcnn_resnet50_fpn_1_epoch', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим что получилось:\n### Let's look at what we got"},{"metadata":{"trusted":false},"cell_type":"code","source":"unnormalize_1 = transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n                                         std=[1, 1, 1])\nunnormalize_2 = transforms.Normalize(mean=[0, 0, 0],\n                                         std=[1/0.229, 1/0.224, 1/0.225])\nunnormalize = transforms.Compose([unnormalize_2, unnormalize_1])\n\nstart = 2\n\nimages = []\nfor i in range(start, start + 2):\n    images.append(val_dataset[i][0].to(device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def detach_dict(pred):\n    return{k:v.detach().cpu() for (k,v) in pred.items()}\n\nmodel.eval()\npreds = model(images)\npreds = [detach_dict(pred) for pred in preds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"fig,ax = plt.subplots(1, 2, figsize = (20, 8))\n\nfor i in range(2):\n    image = unnormalize(images[i].clone().cpu())\n    ax[i].imshow(image.numpy().transpose([1,2,0]))\n    for box in preds[i]['boxes']:\n        box = box.detach().cpu().numpy()\n        rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n        ax[i].add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Отдельная модель, которая по фрагменту фотографии с номером, будет его распозновать (CRNN)\n## 3. CRNN - a separate model for a license plate recognition"},{"metadata":{"trusted":false},"cell_type":"code","source":"class CarPlatesFragmentsDataset(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9, alphabet=abc):\n        super(CarPlatesFragmentsDataset, self).__init__()\n        self.root = Path(root)\n        self.alphabet = alphabet\n        self.train_size = train_size\n        \n        self.image_names = []\n        self.image_ids = []\n        self.image_boxes = []\n        self.image_texts = []\n        self.box_areas = []\n        \n        self.transforms = transforms\n        \n        if split in ['train', 'val']:\n            plates_filename = self.root / 'train.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            train_valid_border = int(len(json_data) * train_size) + 1 # граница между train и valid\n            data_range = (0, train_valid_border) if split == 'train' \\\n                else (train_valid_border, len(json_data))\n            self.load_data(json_data[data_range[0]:data_range[1]]) # загружаем названия файлов и разметку\n            return\n\n        if split == 'test':\n            plates_filename = self.root / 'test_boxes.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            self.load_test_data(json_data)\n            return\n            \n        raise NotImplemented(f'Unknown split: {split}')\n        \n    def load_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            if sample['file'] == 'train/25632.bmp':\n                continue\n            for box in sample['nums']:\n                points = np.array(box['box'])\n                x_0 = np.min([points[0][0], points[3][0]])\n                y_0 = np.min([points[0][1], points[1][1]])\n                x_1 = np.max([points[1][0], points[2][0]])\n                y_1 = np.max([points[2][1], points[3][1]])\n                if x_0 >= x_1 or y_0 >= y_1:\n                    # Есть несколько примеров, когда точки пронумерованы в другом порядке - пока не выясняем\n                    continue\n                if (y_1 - y_0) * 20 < (x_1 - x_0):\n                    continue\n                self.image_boxes.append(np.clip([x_0, y_0, x_1, y_1], a_min=0, a_max=None))\n                self.image_texts.append(box['text'])\n                self.image_names.append(sample['file'])\n        self.revise_texts()\n                \n    def revise_texts(self):\n        wrong = 'АОНКСРВХЕТМУ'\n        correct = 'AOHKCPBXETMY'\n        for i in range(len(self.image_texts)):\n            self.image_texts[i] = self.image_texts[i].upper()\n            for (a, b) in zip(wrong, correct):\n                self.image_texts[i] = self.image_texts[i].replace(a, b)\n            \n                \n    def load_test_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            for box in sample['boxes']:\n                if box[0] >= box[2] or box[1] >= box[3]:\n                    continue\n                points = np.array(box)\n                self.image_boxes.append(np.clip(points, a_min=0, a_max=None))\n                self.image_names.append(sample['file'])\n        self.image_texts = None\n    \n    def __getitem__(self, idx):\n        file_name = self.root / self.image_names[idx]\n        image = cv2.imread(str(file_name))\n        if image is None:\n            file_name = self.image_names[idx]\n            image = cv2.imread(str(file_name))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        text = ''\n        \n        if self.image_boxes is not None:\n            box = self.image_boxes[idx]\n            image = image.copy()[box[1]:box[3], box[0]:box[2]]\n            \n        if self.image_texts is not None:\n            text = self.image_texts[idx]\n            \n        seq = self.text_to_seq(text)\n        seq_len = len(seq)\n\n        output = dict(image=image, seq=seq, seq_len=seq_len, text=text, file_name=file_name)\n        \n        if self.transforms is not None:\n            output = self.transforms(output)\n        \n        return output\n    \n    def text_to_seq(self, text):\n        \"\"\"Encode text to sequence of integers.\n        Accepts string of text.\n        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n        \"\"\"\n        # YOUR CODE HERE\n        seq = [self.alphabet.find(c) + 1 for c in text]\n        return seq\n\n    def __len__(self):\n        return len(self.image_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    \n    def __init__(self, input_size=(64, 320), output_len=20):\n        super(FeatureExtractor, self).__init__()\n        \n        h, w = input_size\n        resnet = getattr(models, 'resnet18')(pretrained=True)\n        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n        \n        self.pool = nn.AvgPool2d(kernel_size=(h // 32, 1))        \n        self.proj = nn.Conv2d(w // 32, output_len, kernel_size=1)\n  \n        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n    \n    def apply_projection(self, x):\n        \"\"\"Use convolution to increase width of a features.\n        Accepts tensor of features (shaped B x C x H x W).\n        Returns new tensor of features (shaped B x C x H x W').\n        \"\"\"\n        # YOUR CODE HERE\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.proj(x)\n        x = x.permute(0, 2, 3, 1).contiguous()\n        return x\n   \n    def forward(self, x):\n        # Apply conv layers\n        features = self.cnn(x)\n        \n        # Pool to make height == 1\n        features = self.pool(features)\n        \n        # Apply projection to increase width\n        features = self.apply_projection(features)\n        \n        return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class SequencePredictor(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n        super(SequencePredictor, self).__init__()\n        \n        self.num_classes = num_classes        \n        self.rnn = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          num_layers=num_layers,\n                          dropout=dropout,\n                          bidirectional=bidirectional)\n        \n        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n        self.fc = nn.Linear(in_features=fc_in,\n                            out_features=num_classes)\n    \n    def _init_hidden_(self, batch_size):\n        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n        Accepts batch size.\n        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n        \"\"\"\n        # YOUR CODE HERE\n        num_directions = 2 if self.rnn.bidirectional else 1\n        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n        \n    def _prepare_features_(self, x):\n        \"\"\"Change dimensions of x to fit RNN expected input.\n        Accepts tensor x shaped (B x (C=1) x H x W).\n        Returns new tensor shaped (W x B x H).\n        \"\"\"\n        # YOUR CODE HERE\n        x = x.squeeze(1)\n        x = x.permute(2, 0, 1)\n        return x\n    \n    def forward(self, x):\n        x = self._prepare_features_(x)\n        \n        batch_size = x.size(1)\n        h_0 = self._init_hidden_(batch_size)\n        h_0 = h_0.to(x.device)\n        x, h = self.rnn(x, h_0)\n        \n        x = self.fc(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions.\n\nclass CRNN(nn.Module):\n    \n    def __init__(self, alphabet=abc,\n                 cnn_input_size=(64, 320), cnn_output_len=20,\n                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n        super(CRNN, self).__init__()\n        self.alphabet = alphabet\n        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n                                                    bidirectional=rnn_bidirectional)\n    \n    def forward(self, x):\n        features = self.features_extractor(x)\n        sequence = self.sequence_predictor(features)\n        return sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def pred_to_string(pred, abc):\n    seq = []\n    for i in range(len(pred)):\n        label = np.argmax(pred[i])\n        seq.append(label - 1)\n    out = []\n    for i in range(len(seq)):\n        if len(out) == 0:\n            if seq[i] != -1:\n                out.append(seq[i])\n        else:\n            if seq[i] != -1 and seq[i] != seq[i - 1]:\n                out.append(seq[i])\n    out = ''.join([abc[c] for c in out])\n    return out\n\ndef decode(pred, abc):\n    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n    outputs = []\n    for i in range(len(pred)):\n        outputs.append(pred_to_string(pred[i], abc))\n    return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class Resize(object):\n\n    def __init__(self, size=(320, 64)):\n        self.size = size\n\n    def __call__(self, item):\n        \"\"\"Accepts item with keys \"image\", \"seq\", \"seq_len\", \"text\".\n        Returns item with image resized to self.size.\n        \"\"\"\n        # YOUR CODE HERE\n        item['image'] = cv2.resize(item['image'], self.size, interpolation=cv2.INTER_AREA)\n        return item","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"crnn = CRNN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"crnn.to(device)\nnum_epochs = 10\nbatch_size = 128\nnum_workers = 4\noptimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"transformations = transforms.Compose([\n    Resize(),\n                    ])\n\ntrain_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'train')\nval_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'val')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n    Accepts list of dataset __get_item__ return values (dicts).\n    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n    \"\"\"\n    images, seqs, seq_lens, texts, file_names = [], [], [], [], []\n    for sample in batch:\n        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n        seqs.extend(sample[\"seq\"])\n        seq_lens.append(sample[\"seq_len\"])\n        texts.append(sample[\"text\"])\n        file_names.append(sample[\"file_name\"])\n    images = torch.stack(images)\n    seqs = torch.Tensor(seqs).int()\n    seq_lens = torch.Tensor(seq_lens).int()\n    \n    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts, \"file_name\": file_names}\n    return batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_plates_dataset, \n                                               batch_size=batch_size, shuffle=True,\n                                               num_workers=num_workers, pin_memory=True, \n                                               drop_last=True, collate_fn=collate_fn)\nval_dataloader = torch.utils.data.DataLoader(val_plates_dataset, \n                                             batch_size=batch_size, shuffle=False,\n                                             num_workers=num_workers, pin_memory=True, \n                                             drop_last=True, collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"crnn.train()\nfor i, epoch in enumerate(range(num_epochs)):\n        epoch_losses = []\n\n        for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n            images = b[\"image\"].to(device)\n            seqs_gt = b[\"seq\"]\n            seq_lens_gt = b[\"seq_len\"]\n\n            seqs_pred = crnn(images).cpu()\n            log_probs = fnn.log_softmax(seqs_pred, dim=2)\n            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n            loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n                                targets=seqs_gt,  # N, S or sum(target_lengths)\n                                input_lengths=seq_lens_pred,  # N\n                                target_lengths=seq_lens_gt)  # N\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_losses.append(loss.item())\n\n        print(i, np.mean(epoch_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val_losses = []\ncrnn.eval()\nfor i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n    images = b[\"image\"].to(device)\n    seqs_gt = b[\"seq\"]\n    seq_lens_gt = b[\"seq_len\"]\n\n    with torch.no_grad():\n        seqs_pred = crnn(images).cpu()\n    log_probs = fnn.log_softmax(seqs_pred, dim=2)\n    seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n    loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n                        targets=seqs_gt,  # N, S or sum(target_lengths)\n                        input_lengths=seq_lens_pred,  # N\n                        target_lengths=seq_lens_gt)  # N\n\n    val_losses.append(loss.item())\n\nprint(np.mean(val_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_ticks = [\"-\"] + [x for x in abc]\n\nimages = b[\"image\"]\nseqs_gt = b[\"seq\"]\nseq_lens_gt = b[\"seq_len\"]\ntexts = b[\"text\"]\n\npreds = crnn(images.to(device)).cpu().detach()\ntexts_pred = decode(preds, crnn.alphabet)\n\nfor i in range(10):\n    plt.figure(figsize=(15, 5))\n    pred_i = preds[:, i, :].T\n\n    plt.subplot(1, 2, 1)\n    image = images[i].permute(1, 2, 0).numpy()\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.title(texts[i])\n\n    plt.subplot(1, 2, 2)\n    plt.yticks(range(pred_i.size(0)), y_ticks)\n    plt.imshow(pred_i)\n    plt.title(texts_pred[i])\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Save\n# with open('crnn_10_epochs.pth', 'wb') as fp:\n#     torch.save(crnn.state_dict(), fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load\nwith open('crnn_10_epochs.pth', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\ncrnn.load_state_dict(state_dict)\ncrnn.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Соберем все вместе и сделаем предсказания для test_data\n## Let'a put everything together and make a prediction on test_data"},{"metadata":{"trusted":false},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Модель для нахождения номерных знаков на фотографиях\n# A model which find bboxes for license plates\nmodel = create_model(device)\n\n# Load\nwith open('fasterrcnn_resnet50_fpn_1_epoch', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.to(device)\n\n# Test dataset\ntest_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'test')\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=2, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_boxes = []\nfor batch in tqdm.tqdm(test_loader):\n    images = list(image.to(device) for image in batch[0])\n    model.eval()\n    preds = model(images)\n    preds = [{k: v.detach().cpu().numpy() for k, v in prediction.items()} for prediction in preds]\n    predicted_boxes.extend(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"boxes = [box.astype(int).tolist() for box in (boxes_in_image['boxes'] for boxes_in_image in predicted_boxes)]\nassert len(boxes) == len(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ### Сохраним результат в файл\n1. ### Save the result"},{"metadata":{"trusted":false},"cell_type":"code","source":"json_data = []\nfor file_name, box in zip(test_dataset.image_names, boxes):\n    json_data.append({'boxes': box, 'file': str(file_name)})\n\nwith open('data/test_boxes.json', 'w') as fp:\n    json.dump(json_data, fp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Датасет рамок номеров из теста\n### Test dataset of license plates bboxes"},{"metadata":{"trusted":false},"cell_type":"code","source":"transformations = transforms.Compose([\n    Resize()])\n\ntest_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_dataloader = torch.utils.data.DataLoader(test_plates_dataset, \n                                              batch_size=1, shuffle=False,\n                                              num_workers=num_workers, pin_memory=True, \n                                              drop_last=True, collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Делаем предсказания\n### Making predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"crnn.eval()\nsubmit = {}\nfor b in tqdm.tqdm(test_dataloader):\n    file_name = b['file_name'][0][5:]\n    if file_name not in submit:\n            submit[file_name] = []\n    images = b[\"image\"]\n    preds = crnn(images.to(device)).cpu().detach()\n    texts_pred = decode(preds, crnn.alphabet)\n    submit[file_name].append(texts_pred[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submit = [(k, ' '.join(v)) for k,v in submit.items()]\nsubmission = pd.DataFrame(submit, columns=['file_name', 'plates_string'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"random_submission = pd.read_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.merge(random_submission, submission, how='left', on='file_name')\nsubmission.drop('plates_string_x', axis=1, inplace=True)\nsubmission.columns = ['file_name', 'plates_string']\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('baseline_crnn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}