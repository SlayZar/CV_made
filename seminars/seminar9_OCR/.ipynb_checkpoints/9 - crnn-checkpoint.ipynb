{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MADE](resources/made.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Академия MADE\n",
    "\n",
    "\n",
    "## Семинар 9: распознавание автомобильных номеров\n",
    "\n",
    "\n",
    "Сегодня мы займемся задачей распознавания автомобильных номеров. Очевидно, она является частным случаем более широкой задачи - распознавания текста на изображениях.\n",
    "\n",
    "Мы не будем затрагивать тему локализации номеров на изображении и сосредоточимся на \"чтении\" текста номерных знаков на уже подготовленных кропах.\n",
    "\n",
    "#### План:\n",
    "1. Общая схема\n",
    "2. Подготовка данных\n",
    "3. Построение модели `CRNN` (`Convolutional Recurrent Neural Network`)\n",
    "4. Интерфейс и применение функции потерь `CTC Loss` (`Connectionist Temporal Classification`)\n",
    "5. Обучение и результаты\n",
    "6. Анализ проблем и что делать дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Общая схема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![box](resources/black_box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу распознавания текста на изображении можно рассматривать под разными углами:\n",
    "- Как классификацию (1 слово = 1 класс);\n",
    "- Как классификацию отдельных символов (то есть сначала разрезать на символы, затем каждый отдельно \"прочитать\");\n",
    "- Как предсказание последовательностей (фиксированной или переменной длины).\n",
    "\n",
    "Первый подход очевидно очень плох. Во-первых, при обучении мы будем иметь дело с очень разреженными данными (вероятно, для каких-то \"классов\"-слов у нас будет только по одному примеру), а в боевых данных могут оказаться слова, которых в обучающей выборке не было вовсе. Во-вторых, это классификация на гигантское число классов - такую модель будет очень непросто обучить. Однако, если в задаче сильно ограничен словарь (множество \"слов\"), то эту схему можно использовать (например, если вы хотите распознавать номера футболистов на форме).\n",
    "\n",
    "Второй подход тоже не лишен недостатков. Самый очевидный - необходимо уметь хорошо разрезать слова на отдельные символы. В случае изображений из реальной жизни возникают дополнительные сложности в виде экзотических шрифтов, не позволяющих адекватно разделять соседние символы, шумов и т.д.\n",
    "\n",
    "Третий подход не требует специальной подготовки данных и прост с точки зрения архитектуры. К нему мы и обратимся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Recurrent Neural Network (CRNN)\n",
    "\n",
    "[paper](https://arxiv.org/abs/1507.05717)\n",
    "\n",
    "![crnn](resources/crnn.png)\n",
    "\n",
    "Модель CRNN (не путайте с R-CNN - семейством детекторов) предназначена для перевода изображений в текстовый вид. Она устроена следующим образом (см.изображение выше):\n",
    "1. Первый этап - извлечение признаков. Входное изображение пропускается через последовательность сверточных слоев (с активациями, пулингом, вот эти вот всем), в результате чего получается тензор признаков размера, например, `C x H x W`. Если рассматривать в отдельности каждый из срезов вдоль оси \"ширины\" (`W`, последняя размерность), то можно сказать, что у нас получилась последовательность длины `W`. Каждый из этих элементов соответствует определенной области на исходном изображении: ![receptive](resources/receptive.png)\n",
    "2. Второй этап - предсказание вероятностей каждого из символов алфавита **для каждого элемента последовательности**. Полученная на первом шаге последовательность признаков пропускается через рекуррентную сеть, в результате чего на выходе образуется последовательность той же длины, что и на входе. Размер каждого элемента равен мощности алфавита + 1 (этот \"+1\" - символ `blank` для использования `CTC-Loss`).\n",
    "3. Третий этап - либо декодирование (\"транскрипция\") полученных на предыдущем шаге распределений в итоговый текст (при инференсе), либо вычисление `CTC-Loss` между полученными распределениями и `ground-truth`-последовательностями.\n",
    "\n",
    "Быстрый FAQ:\n",
    "* Q: Длина предсказанных последовательностей после первого шага фиксирована?\n",
    "  \n",
    "  A: Да, если используются батчи с изображениями одинакового размера.\n",
    "\n",
    "* Q: Как из \"двумерных\" элементов после первого шага получаются \"столбики\" (как на картинке выше)?\n",
    "  \n",
    "  A: Можно сделать разворачивание в столбец (размер элементов станет -> `C * H`), пулинг по высоте (-> `C`), линейный слой, <ваш вариант>, ...\n",
    "\n",
    "* Q: Длина предсказанных последовательностй после второго шага равна длине предсказанных в итоге слов?\n",
    "\n",
    "  A: Нет, ее следует сделать больше. На последнем шаге используется `CTC-Loss`, который умеет \"сжимать\" предсказанные последовательности (в том числе с помощью `blank`-символов).\n",
    "  \n",
    "  \n",
    "* Q: Как работает `CTC-Loss`?\n",
    "\n",
    "  A: Пересмотрите лекцию, читайте [статью](https://www.cs.toronto.edu/~graves/icml_2006.pdf), смотрите [видео](https://www.youtube.com/watch?v=eYIL4TMAeRI), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/gm9cqe385h3fjlh/crnn.pth.tar?dl=0 crnn.pth.tar\n",
    "!wget https://www.dropbox.com/s/0imjz3608l3famy/images.zip \n",
    "!unzip images.zip\n",
    "!rm images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision import models\n",
    "\n",
    "from string import digits, ascii_uppercase\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет представляет собой размеченные изображения автомобильных номеров РФ, собранных из интернета либо сгенерированных автоматически.\n",
    "Формат данных = папка с изображениями + файл конфигурации, в котором в виде списка хранятся записи о тексте на каждом из изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"/data2/made/datasets/carocr-seminar/\"  # Change to your path with unzipped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(PATH_TO_DATA, \"config.json\")\n",
    "images_path = os.path.join(PATH_TO_DATA, \"images\")\n",
    "assert os.path.isfile(config_path)\n",
    "assert os.path.isdir(images_path)\n",
    "\n",
    "with open(config_path, \"rt\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "config_full_paths = []\n",
    "for item in config:\n",
    "    config_full_paths.append({\"file\": os.path.join(images_path, item[\"file\"]),\n",
    "                              \"text\": item[\"text\"]})\n",
    "config = config_full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total items in data:\", len(config))\n",
    "print(\"First 3 items:\")\n",
    "for item in config[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 30\n",
    "NUM_COLS = 5\n",
    "NUM_ROWS = SAMPLE_SIZE // NUM_COLS + int(SAMPLE_SIZE % NUM_COLS != 0)\n",
    "\n",
    "random_idxs = np.random.choice(len(config), size=SAMPLE_SIZE, replace=False)\n",
    "plt.figure(figsize=(20, 2 * NUM_ROWS))\n",
    "for i, idx in enumerate(random_idxs, 1):\n",
    "    item = config[idx]\n",
    "    text = item[\"text\"]\n",
    "    image = cv2.imread(item[\"file\"])\n",
    "    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    plt.imshow(image[:, :, ::-1])\n",
    "    plt.title(text)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в данных встречаются номера разных типов. Для учебных целей отфильтруем примеры и оставим только те, которые относятся к \"стандартным\" гражданским номерным знакам, а именно имеющим вид `LDDDLLDD` или `LDDDLLDDD` (`L` = \"letter\", то есть буква, `D` = \"digit\", цифра).\n",
    "Кроме того, отфильтруем примеры, в которые встречаются символы, не входящие в алфавит регистрационных знаков (см. переменную `abc` ниже).\n",
    "\n",
    "**NB: используются только заглавные буквы латинского алфавита и цифры.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ascii_uppercase)  # may be useful for functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits)  # may be useful for functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(text):\n",
    "    \"\"\"Compute letter-digit mask of text.\n",
    "    Accepts string of text. \n",
    "    Returns string of the same length but with every letter replaced by 'L' and every digit replaced by 'D'.\n",
    "    e.g. 'E506EC152' -> 'LDDDLLDDD'.\n",
    "    Returns None if non-letter and non-digit character met in text.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return\n",
    "\n",
    "assert compute_mask(\"E506EC152\") == \"LDDDLLDDD\"\n",
    "assert compute_mask(\"E123KX99\") == \"LDDDLLDD\"\n",
    "assert compute_mask(\"P@@@KA@@\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_alphabet(text, alphabet=abc):\n",
    "    \"\"\"Check if all chars in text come from alphabet.\n",
    "    Accepts string of text and string of alphabet. \n",
    "    Returns True if all chars in text are from alphabet and False else.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return\n",
    "\n",
    "assert check_in_alphabet(\"E506EC152\") is True\n",
    "assert check_in_alphabet(\"A123GG999\") is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(config):\n",
    "    \"\"\"Filter config keeping only items with correct text.\n",
    "    Accepts list of items.\n",
    "    Returns new list.\n",
    "    \"\"\"\n",
    "    config_filtered = []\n",
    "    for item in tqdm.tqdm(config):\n",
    "        text = item[\"text\"]\n",
    "        mask = compute_mask(text)\n",
    "        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n",
    "            config_filtered.append({\"file\": item[\"file\"],\n",
    "                                    \"text\": item[\"text\"]})\n",
    "    return config_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = filter_data(config)\n",
    "print(\"Total items in data after filtering:\", len(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что осталось после фильтрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 30\n",
    "NUM_COLS = 5\n",
    "NUM_ROWS = SAMPLE_SIZE // NUM_COLS + int(SAMPLE_SIZE % NUM_COLS != 0)\n",
    "\n",
    "random_idxs = np.random.choice(len(config), size=SAMPLE_SIZE, replace=False)\n",
    "plt.figure(figsize=(20, 2 * NUM_ROWS))\n",
    "for i, idx in enumerate(random_idxs, 1):\n",
    "    item = config[idx]\n",
    "    text = item[\"text\"]\n",
    "    image = cv2.imread(item[\"file\"])\n",
    "    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    plt.imshow(image[:, :, ::-1])\n",
    "    plt.title(text)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к созданию класса датасета. Что важно:\n",
    "- Конструктор принимает список словарей с ключами `file` и `text` (`config`), строку с алфавитом для предсказания (`alphabet`) и трансформации (`transforms`)\n",
    "- Для обучения нам потребуется возвращать в методе `__getitem__`:\n",
    "  - Изображение номера (фиксированного размера)\n",
    "  - Текст номера в виде числовой последовательности (т.е. в закодированном виде)\n",
    "  - Длину этой последовательности (требование для обучения с `CTC Loss`)\n",
    "  - Текст в виде строки (для удобства)\n",
    "  \n",
    "  Удобно сложить все эти переменные в словарь и доставать их оттуда по ключам при необходимости `->` `transforms` должны работать со словарем!\n",
    "- Отображение \"текст `<->` числовая последовательность\" будем делать простым индексированием по строке алфавита. Число \"0\" зарезервируем для символа `blank`. \n",
    "  - Например, пусть наш алфавит = `XYZ`. Тогда текст `XXZY` будет представлена как `[1,1,3,2]` (без `blank` было бы `[0,0,2,1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, config, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        Accepts:\n",
    "        - config: list of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "        - alphabet: string of chars required for predicting.\n",
    "        - transforms: transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDataset, self).__init__()\n",
    "        self.config = config\n",
    "        self.alphabet = abc\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        for item in self.config:\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.image_names[item]).astype(np.float32) / 255.\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "        Accepts string of text.\n",
    "        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве трансформации будем использовать только ресайз к фиксированному размеру `(320, 64)`. \n",
    "\n",
    "**NB**: не забудьте про интерполяцию ([stackoverflow](https://stackoverflow.com/questions/3112364/how-do-i-choose-an-image-interpolation-method-emgu-opencv))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Accepts item with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        Returns item with image resized to self.size.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем датасет вместе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Resize(size=(320, 64))\n",
    "dataset = RecognitionDataset(config, alphabet=abc, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[0]\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image shape:\", x[\"image\"].shape)\n",
    "print(\"Seq:\", x[\"seq\"], \"Seq_len:\", x[\"seq_len\"])\n",
    "print(\"Text:\", x[\"text\"])\n",
    "plt.imshow(x[\"image\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст номеров может иметь длину 8 (`LDDDLLDD`) или 9 (`LDDDLLDDD`). Класс `DataLoader` плохо справляется (из коробки) с данными переменного размера, поэтому нам придется написать свою реализацию функции `collate_fn`, чтобы `DataLoader` понял, как формировать батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    Accepts list of dataset __get_item__ return values (dicts).\n",
    "    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for sample in batch:\n",
    "        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(sample[\"seq\"])\n",
    "        seq_lens.append(sample[\"seq_len\"])\n",
    "        texts.append(sample[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [dataset[i] for i in range(4)]\n",
    "batch = collate_fn(xs)\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image:\", batch[\"image\"].size())\n",
    "print(\"Seq:\", batch[\"seq\"].size())\n",
    "print(\"Seq:\", batch[\"seq\"])\n",
    "print(\"Seq_len:\", batch[\"seq_len\"])\n",
    "print(\"Text:\", batch[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Построение модели CRNN-like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к построению класса для модели нейросети.\n",
    "Следуя описанию в начале этой страницы, вынесем (1) и (2) этапы пайплайна в отдельные компоненты модели (`self.cnn` и `self.rnn` соответсвенно).\n",
    "Их можно реализовать практически независимо друг от друга, поэтому после занятия вы сможете самостоятельно поэкспериментировать со своими вариантами архитектур."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с построения `feature_extractor`. \n",
    "\n",
    "Для этого возьмем предобученную модель `ResNet18`, отрежем от нее последние два слоя (это `AdaptiveAvgPool2d` и `Linear`), оставив только полносверточную часть. После всех сверточных слоев размер входного изображения уменьшается в 32 раза, а значит, входная картинка размером `64x320` превратится в тензор с высотой и шириной `2x10`. Иными словами, длина последовательности для подачи в `RNN` составляет всего лишь 10 (этого может быть мало для хорошей работы `CTC-Loss`). Используем трюк (в методе `apply_projection`), применив сверточный слой с ядром (1x1) вдоль размерности ширины (а не глубины, как обычно), увеличив длину последовательности с 10 до `output_len`.\n",
    "\n",
    "**NB**: реализуйте метод `apply_projection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Module):\n",
    "    \n",
    "    def __init__(self, input_size=(64, 320), output_len=20):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, 'resnet18')(pretrained=True)\n",
    "        self.cnn = Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.pool = AvgPool2d(kernel_size=(h // 32, 1))        \n",
    "        self.proj = Conv2d(w // 32, output_len, kernel_size=1)\n",
    "  \n",
    "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
    "    \n",
    "    def apply_projection(self, x):\n",
    "        \"\"\"Use convolution to increase width of a features.\n",
    "        Accepts tensor of features (shaped B x C x H x W).\n",
    "        Returns new tensor of features (shaped B x C x H x W').\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Apply conv layers\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Pool to make height == 1\n",
    "        features = self.pool(features)\n",
    "        \n",
    "        # Apply projection to increase width\n",
    "        features = self.apply_projection(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = feature_extractor(x)\n",
    "assert y.size() == (1, 1, 512, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем класс для рекуррентной части пайплайна.\n",
    "\n",
    "Будем использовать `GRU` (`bidirectional` или нет - выбор по параметру). Результат предсказаний после `GRU` дополнительно проведем через линейный слой для формирования итоговой матрицы с `logits`. \n",
    "\n",
    "**NB:** Для инициализации скрытого состояния `GRU` реализуйте метод `_init_hidden_()`. Про размерность `hidden_state` можно [посмотреть в документации](https://pytorch.org/docs/stable/nn.html?highlight=gru#torch.nn.GRU).\n",
    "\n",
    "**NB:** `GRU` по умолчанию ожидает на вход тензор размера `L x B x F`, где `L` - длина последовательности, `B` - размер батча, `F` - размер одного элемента последовательности. Реализуйте приведение тензора из `FeatureExtractor` в этот вид в функции `_prepare_features_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredictor(Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.rnn = GRU(input_size=input_size,\n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout,\n",
    "                       bidirectional=bidirectional)\n",
    "        \n",
    "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
    "        self.fc = Linear(in_features=fc_in,\n",
    "                         out_features=num_classes)\n",
    "    \n",
    "    def _init_hidden_(self, batch_size):\n",
    "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
    "        Accepts batch size.\n",
    "        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "        \n",
    "    def _prepare_features_(self, x):\n",
    "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
    "        Accepts tensor x shaped (B x (C=1) x H x W).\n",
    "        Returns new tensor shaped (W x B x H).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._prepare_features_(x)\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        h_0 = self._init_hidden_(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        x, h = self.rnn(x, h_0)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_predictor = SequencePredictor(input_size=512, \n",
    "                                       hidden_size=128, \n",
    "                                       num_layers=2, \n",
    "                                       num_classes=len(abc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 512, 20)\n",
    "assert sequence_predictor._prepare_features_(x).size() == (20, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequence_predictor(x)\n",
    "assert y.size() == (20, 1, 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь соберем две части в один класс CRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    \n",
    "    def __init__(self, alphabet=abc,\n",
    "                 cnn_input_size=(64, 320), cnn_output_len=20,\n",
    "                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n",
    "        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n",
    "                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n",
    "                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n",
    "                                                    bidirectional=rnn_bidirectional)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features_extractor(x)\n",
    "        sequence = self.sequence_predictor(features)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, добавим также функции для декодирования результата `sequence_predictor` в читаемый вид."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_string(pred, abc):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([abc[c] for c in out])\n",
    "    return out\n",
    "\n",
    "def decode(pred, abc):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], abc))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = crnn(x)\n",
    "assert y.size() == (20, 1, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(y, abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CTC-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели предсказания последовательностей будем использовать `CTC-Loss`. Класс этой функции потерь уже реализован в `PyTorch`, поэтому нам нужно только понять, как правильно подать в него предсказания и `ground-truth`-метки. Для этого обратимся к [документации](https://pytorch.org/docs/stable/nn.functional.html?highlight=ctc#torch.nn.functional.ctc_loss):\n",
    "\n",
    "![ctc-01](resources/ctc_01.png)\n",
    "![ctc-02](resources/ctc_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На что следует обратить внимание:\n",
    "* Функция ожидает на вход не только пару предсказанных и верных последовательностей, но и информацию о длинах этих последовательностей.\n",
    "* Перед тем, как подавать предсказания в лосс, необходимо применить к ним активацию `softmax` и затем взять логарифм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Обучение и результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше никаких `#YOUR CODE HERE` не будет - если все необходимые функции выше реализованы, то можно запускать ячейки ниже и следить за обучением.\n",
    "\n",
    "Можно выставить константу `ACTUALLY_TRAIN=False`, тогда вместо обучения будут загружены логи предварительно выполненного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUALLY_TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель (пока все параметры можно оставить по умолчанию - они подойдут для начала):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda: 0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "crnn.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделим данные на обучающую и валидационную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(config)\n",
    "train_size = int(len(config) * 0.8)\n",
    "config_train = config[:train_size]\n",
    "config_val = config[train_size:]\n",
    "\n",
    "train_dataset = RecognitionDataset(config_train, transforms=Resize())\n",
    "val_dataset = RecognitionDataset(config_val, transforms=Resize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объекты типа `DataLoader` для формирования батчей обучения. Обратите внимание на следующие вещи:\n",
    "* Мы передаем функцию `collate_fn` как параметр конструктора;\n",
    "* Значения параметров `shuffle` и `drop_last` отличаются для случаев обучения и валидации - зачем так сделано?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, \n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "                            drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение (при параметрах по умолчанию ~30 минут):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.train()\n",
    "if ACTUALLY_TRAIN:\n",
    "    for i, epoch in enumerate(range(num_epochs)):\n",
    "        epoch_losses = []\n",
    "\n",
    "        for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "            images = b[\"image\"].to(device)\n",
    "            seqs_gt = b[\"seq\"]\n",
    "            seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "            seqs_pred = crnn(images).cpu()\n",
    "            log_probs = log_softmax(seqs_pred, dim=2)\n",
    "            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "            loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        print(i, np.mean(epoch_losses))\n",
    "else:\n",
    "    image_train_log = cv2.imread(\"./resources/train_log.png\")\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    plt.imshow(image_train_log[:, :, ::-1], interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Провалидируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.eval()\n",
    "if ACTUALLY_TRAIN:\n",
    "    val_losses = []\n",
    "    for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seqs_pred = crnn(images).cpu()\n",
    "        log_probs = log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                        targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                        input_lengths=seq_lens_pred,  # N\n",
    "                        target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    print(np.mean(val_losses))\n",
    "else:\n",
    "    image_val_log = cv2.imread(\"./resources/val_log.png\")\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    plt.imshow(image_val_log[:, :, ::-1], interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем несколько картинок из валидации и посмотрим на предсказанные для них распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ACTUALLY_TRAIN:\n",
    "    \n",
    "    y_ticks = [\"-\"] + [x for x in abc]\n",
    "\n",
    "    images = b[\"image\"]\n",
    "    seqs_gt = b[\"seq\"]\n",
    "    seq_lens_gt = b[\"seq_len\"]\n",
    "    texts = b[\"text\"]\n",
    "    \n",
    "    preds = crnn(images.to(device)).cpu().detach()\n",
    "    texts_pred = decode(preds, crnn.alphabet)\n",
    "\n",
    "    for i in range(10):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        pred_i = preds[:, i, :].T\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        image = images[i].permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(texts[i])\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.yticks(range(pred_i.size(0)), y_ticks)\n",
    "        plt.imshow(pred_i)\n",
    "        plt.title(texts_pred[i])\n",
    "        \n",
    "        plt.show()\n",
    "else:\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        image_name = f\"./resources/{str(i).zfill(2)}.png\"\n",
    "        image = cv2.imread(image_name)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(image[:, :, ::-1], interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Что делать дальше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- При обучении и валидации мы смотрели только на значение функции потерь. Обычно принято для оценки качества моделей использовать какую-либо репрезентативную метрику. Реализуйте подсчет `Accuracy` (доля верно предсказанных номеров) на обучающей и валидационной выборке.\n",
    "- Как можно видеть по последним картинкам, наша модель почти ничего не предсказывает в середине последовательности (предсказывает `blank`). Попробуйте изменить архитктуру (или поиграйтесь с параметрами текущей) модели, чтобы более эффективно использовать ее.\n",
    "- Проанализируйте, на каких примерах валидационной (и обучающей) выборки модель ошибается? Что можно сделать, чтобы полечить эти проблемы?\n",
    "- Мы не умеем оценивать, насколько адекватны предсказания модели. Подумайте, как можно оценить \"уверенность\" модели в собственных предсказаниях?\n",
    "- Вспомните, что мы оставили в выборке только \"стандартные\" номера. Попробуйте вернуть в обучение более сложные классы и обучить модель. Что получится?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
